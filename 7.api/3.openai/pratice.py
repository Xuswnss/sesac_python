from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv

load_dotenv()

# ê¸°ë³¸ê°’ìœ¼ë¡œ OpenAI ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
embeddings = OpenAIEmbeddings()

# # í…ìŠ¤íŠ¸ ë¬¸ìì—´ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
# texts = ["Hello, how are you?", "I love programming."]
# vector_embeddings = embeddings.embed_documents(texts)  # âœ… ë³€ê²½ë¨

# ê²°ê³¼ ì¶œë ¥
# for text, vector in zip(texts, vector_embeddings):
#     print(f"Text: {text}")
#     print(f"Embedding length: {len(vector)}")  # ë²¡í„° ê¸¸ì´ ì¶œë ¥
#     print(f"Embedding (first 5 values): {vector[:5]}\n")  # ì• 5ê°œ ê°’ë§Œ ì¶œë ¥

from langchain_core.runnables import RunnablePassthrough

from langchain_core.runnables import RunnablePassthrough

# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ëŠ” í•¨ìˆ˜ ì •ì˜
# def user_input(input_value):
#     return f"ì…ë ¥ëœ ê°’: {input_value}"

# # RunnablePassthrough ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# passthrough = RunnablePassthrough()

# # ì²˜ë¦¬ ì²´ì¸ êµ¬ì„±
# chain = passthrough | user_input
# print('### chain  : ' , chain)

# # ì…ë ¥ ê°’ ì •ì˜
# input_value = "ì•ˆë…•í•˜ì„¸ìš”, ì´ ë©”ì‹œì§€ëŠ” ë³€í˜•ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

# # ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•˜ì—¬ ì¶œë ¥
# output = chain.invoke(input_value)
# print("ì¶œë ¥:", output)

# 2-íŠœí”Œ í˜•íƒœì˜ ë©”ì‹œì§€ ëª©ë¡ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„± (type, content)

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# # 1. í”„ë¡¬í”„íŠ¸ ì •ì˜ (2-íŠœí”Œ í˜•ì‹)
# chat_prompt = ChatPromptTemplate.from_messages([
#     ("system", "ì´ ì‹œìŠ¤í…œì€ ì²œë¬¸í•™ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."),
#     ("user", "{user_input}"),
# ])

# # 2. LLM ì´ˆê¸°í™”
# llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# # 3. ì…ë ¥ëœ ë©”ì‹œì§€ êµ¬ì¡° í™•ì¸
# formatted_messages = chat_prompt.format_messages(user_input="íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?")
# for msg in formatted_messages:
#     print(f"{msg.__class__.__name__}: {msg.content}")

# print("\n--- LLM ì‘ë‹µ ---")

# # 4. ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
# chain = chat_prompt | llm | StrOutputParser()
# print(chain)
# result = chain.invoke({"user_input": "íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?"})


# from langchain_openai import ChatOpenAI
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser

# # prompt + model + output parser
# prompt = ChatPromptTemplate.from_template("You are an expert in astronomy. Answer the question. <Question>: {input}")
# llm = ChatOpenAI(model="gpt-4o-mini")
# chain = prompt | llm 
# result = chain.invoke({"input": "ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ”?"})
# print(f'###### Before strOutParser() : {result}, type : {type(result)}')


# output_parser = StrOutputParser()
# chain = prompt | llm | output_parser
# result = chain.invoke({"input": "ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ”?"})
# print(f'###### After strOutParser() : {result}, type : {type(result)}')


# from langchain_core.documents import Document

# document = Document("ì•ˆë…•í•˜ì„¸ìš” ê³µë°±ì˜¤ì…ë‹ˆë‹¤.")
# # ë©”íƒ€ë°ì´í„° ì¶”ê°€
# document.metadata["source"] = "0105"
# document.metadata["page"] = 1
# document.metadata["author"] = "ê³µë°±ì˜¤"
# print('document.page_content:',document.page_content)
# print('document.metadata :',document.metadata)


from langchain_core.prompts import PromptTemplate

# # 'name'ê³¼ 'age'ë¼ëŠ” ë‘ ê°œì˜ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì •ì˜
# template_text = "ì•ˆë…•í•˜ì„¸ìš”, ì œ ì´ë¦„ì€ {name}ì´ê³ , ë‚˜ì´ëŠ” {age}ì‚´ì…ë‹ˆë‹¤."

# # PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±
# prompt_template = PromptTemplate.from_template(template_text)

# # í…œí”Œë¦¿ì— ê°’ì„ ì±„ì›Œì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±
# filled_prompt = prompt_template.format(name="ê³µë°±ì˜¤", age=22)

# print(filled_prompt)

from langchain_core.output_parsers import CommaSeparatedListOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# # 1. ì¶œë ¥ íŒŒì„œ ì´ˆê¸°í™” (ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
# output_parser = CommaSeparatedListOutputParser()

# # 2. ì¶œë ¥ í˜•ì‹ ì§€ì¹¨ ë°›ê¸°
# format_instructions = output_parser.get_format_instructions()

# # 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (í˜•ì‹ ì§€ì¹¨ í¬í•¨)
# prompt = PromptTemplate(
#     template="List five {subject}.\n{format_instructions}",
#     input_variables=["subject"],
#     partial_variables={"format_instructions": format_instructions},
# )

# # 4. LLM ëª¨ë¸ ì´ˆê¸°í™”
# model = ChatOpenAI(temperature=0)

# # 5. ì²´ì¸ ìƒì„±: í”„ë¡¬í”„íŠ¸ â†’ ëª¨ë¸ â†’ ì¶œë ¥ íŒŒì„œ
# chain = prompt | model | output_parser

# # 6. ì‹¤í–‰: 'subject' ë³€ìˆ˜ì— ëŒ€í•´ í˜¸ì¶œ
# result = chain.invoke({"subject": "idols"})
# print("ê²°ê³¼ ë¦¬ìŠ¤íŠ¸:", result)

# import requests
# import os

# API_KEY = os.getenv('OPENAI_API_KEY') # OpenAI API í‚¤ ì…ë ¥
# url = "https://api.openai.com/v1/chat/completions"

# headers = {
#     "Content-Type": "application/json",
#     "Authorization": f"Bearer {API_KEY}"
# }

# data = {
#     "model": "gpt-4o-mini",
#     "messages": [
#         {"role": "user", "content": "ë„ˆ ê³µë°±ì˜¤ë¼ê³  ì•Œì•„?? í‹°ìŠ¤í† ë¦¬ í•˜ëŠ”ë° ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³ í•´.. ì†”ì§íˆë§í•´ì•¼ë˜ê³  ì•Œê³  ìˆìœ¼ë©´ ì–´ë–¤ ì£¼ì œì˜ ë¸”ë¡œê·¸ì¸ì§€ ì•Œë ¤ì¤˜"}
#     ]
# }

# response = requests.post(url, headers=headers, json=data)
# result = response.json()

# print("ğŸ’¬ GPT ì‘ë‹µ:", result["choices"][0]["message"]["content"])

from openai import OpenAI


# client = OpenAI()

# response = client.chat.completions.create(
#     model="gpt-4o-mini",
#     messages=[
#         {"role": "user", "content": "ê³µë°±ì˜¤ ë¸”ë¡œê·¸ êµ¬ë…ì ìˆ˜ëŠ”? ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  í•´!"}
#     ]
# )

# print("ğŸ’¬ GPT ì‘ë‹µ:", response.choices[0].message.content)
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage



# OpenAI LLM ê°ì²´ ìƒì„±
llm = ChatOpenAI(model="gpt-4o-mini")

# ëŒ€í™” ë©”ì‹œì§€ ì „ë‹¬
response = llm([HumanMessage(content="ì˜¬ë°ì´ í”„ë¡œì íŠ¸ ë„ˆë¬´ ì¢‹ì•„! ë„ˆë„ ê·¸ë ‡ë‹ˆ?")])

print("ğŸ’¬ GPT ì‘ë‹µ:", response.content)